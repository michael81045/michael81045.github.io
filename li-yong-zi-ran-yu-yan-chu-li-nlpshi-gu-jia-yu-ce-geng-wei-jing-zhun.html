
<!DOCTYPE html>
<html lang="zh-hant-tw">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="/theme/pygments/github.min.css">


  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/solid.css">

    <link href="/static/custom.css" rel="stylesheet">






<meta name="author" content="Michael Chien" />
<meta name="description" content="本篇結合自然語言處理的相關技術與統計資料分析為2019年選修資工所類神經網路課程時所做的side project。 前言 （2020/08/13更新） 股價的漲跌往往不是我們這些股市菜鳥所能夠理解的，尤其是在股價的起伏還受到各種消息面的影響，甚至老闆的一句話就可以讓股價一飛沖天或跌落谷底，近年來最著名的例子就是特斯拉的執行長 Elon Musk，若沒有掌握到這些消息將會對股價的漲幅丈二金剛摸不著頭緒。 img+br+em,a+br+em, img+em, a+em { font-size: 90%; /*使用較小的文字*/ font-color: #AAAAAA; /*使用較淡的文字色彩*/ font-style: normal; /*以正常字體呈現*/ } Elon Musk 總是喜歡在Twitter上發表個人看法，他的一言一行牽動著旗下公司股價的漲跌 ( 圖片來源 ) 有些股市老手會說：哎呀花時間閱讀那些資訊是沒有用的，等你看到時早就已經反映在股價上了，但事實上可能有些資訊早就潛藏在其中，只是礙於我們的經驗不足而無法做出適當的判斷。 因此本篇將透過自然語言處理（Natural Language Processing, NLP）試著將公司行號釋出的重大消息資訊納入，透過科技來輔佐我們在茫茫的資訊海中抽絲剝繭，以期望能輔助我們尋找出其中的脈絡並先行做出部署 …" />
<meta name="keywords" content="Natural Language Processing, Stock">


<meta property="og:site_name" content="Learning note & Build some side projects | Michael Chien"/>
<meta property="og:title" content="利用自然語言處理(NLP)使股價預測更為精準"/>
<meta property="og:description" content="本篇結合自然語言處理的相關技術與統計資料分析為2019年選修資工所類神經網路課程時所做的side project。 前言 （2020/08/13更新） 股價的漲跌往往不是我們這些股市菜鳥所能夠理解的，尤其是在股價的起伏還受到各種消息面的影響，甚至老闆的一句話就可以讓股價一飛沖天或跌落谷底，近年來最著名的例子就是特斯拉的執行長 Elon Musk，若沒有掌握到這些消息將會對股價的漲幅丈二金剛摸不著頭緒。 img+br+em,a+br+em, img+em, a+em { font-size: 90%; /*使用較小的文字*/ font-color: #AAAAAA; /*使用較淡的文字色彩*/ font-style: normal; /*以正常字體呈現*/ } Elon Musk 總是喜歡在Twitter上發表個人看法，他的一言一行牽動著旗下公司股價的漲跌 ( 圖片來源 ) 有些股市老手會說：哎呀花時間閱讀那些資訊是沒有用的，等你看到時早就已經反映在股價上了，但事實上可能有些資訊早就潛藏在其中，只是礙於我們的經驗不足而無法做出適當的判斷。 因此本篇將透過自然語言處理（Natural Language Processing, NLP）試著將公司行號釋出的重大消息資訊納入，透過科技來輔佐我們在茫茫的資訊海中抽絲剝繭，以期望能輔助我們尋找出其中的脈絡並先行做出部署 …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/li-yong-zi-ran-yu-yan-chu-li-nlpshi-gu-jia-yu-ce-geng-wei-jing-zhun.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-06-05 11:20:00+08:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/michael-chien.html">
<meta property="article:section" content="articles"/>
<meta property="article:tag" content="Natural Language Processing"/>
<meta property="article:tag" content="Stock"/>
<meta property="og:image" content="static/profile.jpg">

  <title>Learning note & Build some side projects | Michael Chien &ndash; 利用自然語言處理(NLP)使股價預測更為精準</title>

</head>
<body class="light-theme">
  <aside>
    <div>
      <a href="">
        <img src="static/profile.jpg" alt="Michael Chien" title="Michael Chien">
      </a>

      <h1>
        <a href="">Michael Chien</a>
      </h1>

<p>Data enthusiast / Database / ML <br/> <br/> 投入巨量資料分析相關研究 <br/> 熱衷於挖掘資料數據中的價值</p>

      <nav>
        <ul class="list">


              <li>
                <a target="_self"
                   href="/pages/about.html#about">
                  about
                </a>
              </li>

        </ul>
      </nav>

      <ul class="social">
          <li>
            <a  class="sc-github" href="https://github.com/michael81045" target="_blank">
              <i class="fab fa-github"></i>
            </a>
          </li>
          <li>
            <a  class="sc-Another social link" href="#" target="_blank">
              <i class="fab fa-Another social link"></i>
            </a>
          </li>
      </ul>
    </div>

  </aside>
  <main>

    <nav>

      <a href="/index.html">Home</a>
      <a href="/archives.html">Archives</a>
      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>


    </nav>

<article class="single">
  <header>
      
    <h1 id="li-yong-zi-ran-yu-yan-chu-li-nlpshi-gu-jia-yu-ce-geng-wei-jing-zhun">利用自然語言處理(NLP)使股價預測更為精準</h1>
    <p>
      Posted on 週三 05 六月 2019 in <a href="/category/articles.html">articles</a>

    </p>
  </header>


  <div>
    <p></br></p>
<p><strong>本篇結合自然語言處理的相關技術與統計資料分析為2019年選修資工所類神經網路課程時所做的side project。</strong></p>
<p></br></p>
<h1><font size = 5 > <strong>前言</strong> </font></h1>
<p>（2020/08/13更新）</p>
<p>股價的漲跌往往不是我們這些股市菜鳥所能夠理解的，尤其是在股價的起伏還受到各種消息面的影響，甚至老闆的一句話就可以讓股價一飛沖天或跌落谷底，近年來最著名的例子就是特斯拉的執行長 Elon Musk，若沒有掌握到這些消息將會對股價的漲幅丈二金剛摸不著頭緒。</p>
<style>
 img+br+em,a+br+em, img+em, a+em {
     font-size: 90%;  /*使用較小的文字*/
     font-color: #AAAAAA; /*使用較淡的文字色彩*/
     font-style: normal; /*以正常字體呈現*/
 } 
</style>

<p></br></p>
<p align="center">

  <img src="../extra/musk.png">
  <font color = #AAAAAA> Elon Musk 總是喜歡在Twitter上發表個人看法，他的一言一行牽動著旗下公司股價的漲跌 ( 
  <a href="https://technews.tw/?p=596503">圖片來源</a> )
  </font>

</p>

<p></br></p>
<p>有些股市老手會說：哎呀花時間閱讀那些資訊是沒有用的，等你看到時早就已經反映在股價上了，但事實上可能有些資訊早就潛藏在其中，只是礙於我們的經驗不足而無法做出適當的判斷。</p>
<p>因此本篇將透過自然語言處理（Natural Language Processing, NLP）試著將公司行號釋出的重大消息資訊納入，透過科技來輔佐我們在茫茫的資訊海中抽絲剝繭，以期望能輔助我們尋找出其中的脈絡並先行做出部署，來預測股價的漲跌。</p>
<p></br></p>
<h1><font size = 5 > <strong>資料來源</strong> </font></h1>
<p>首先透過爬蟲（web crawler）的方式取得本篇拿來分析用的歷史股價－玉晶光（3406）。</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span> 
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">datetime</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://query1.finance.yahoo.com/v7/finance/download/3406.TW?period1=0&amp;period2=1549258857&amp;interval=1d&amp;events=history&amp;crumb=hP2rOschxO0&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;file.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">writelines</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;file.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">Close</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

<span class="n">stock_TW</span><span class="o">=</span><span class="n">crawl_price</span><span class="p">(</span><span class="s2">&quot;3406.TW&quot;</span><span class="p">)</span>
<span class="n">stock_TW</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;3406.csv&quot;</span><span class="p">)</span>

<span class="n">stock_TW</span>
</pre></div>


<p><img align = "left" witdh = 1000 src= "../extra/stock_3406.png"></p>
<p></br></br></br></br></br></br></br></br></p>
<p>再來我們一樣透過爬蟲的方式將<a href="https://mops.twse.com.tw/">公開資訊網站</a>內過去曾發布過的<strong>重大消息與公告</strong>給抓取下來。</p>
<p>抓到的內容如下（以其中一篇重大消息為例）:</p>
<blockquote>
<p>1.事實發生日:108/05/14
2.接受資金貸與之:
(1)公司名稱:玉晶光電(廈門)有限公司
(2)與資金貸與他人公司之關係:
聯屬關係
(3)資金貸與之限額(仟元):541617
(4)原資金貸與之餘額(仟元):289440
(5)本次新增資金貸與之金額(仟元):62000
(6)是否為董事會授權董事長對同一貸與對象分次撥貸或循環動用之資金貸與:否
(7)迄事實發生日止資金貸與餘額(仟元):351440
(8)本次新增資金貸與之原因:
為因應玉晶光電(廈門)有限公司營運周轉而從事之資金貸與，並供做資金
貸與借新還舊之用。
3.接受資金貸與公司所提供擔保品之:
(1)內容:
無
(2)價值(仟元):0
4.接受資金貸與公司最近期財務報表之:
(1)資本(仟元):4620848
(2)累積盈虧金額(仟元):1403737
5.計息方式:
年利率不低於同幣別借款之最高利率，到期一次付息。
6.還款之:
(1)條件:
視接受資金貸與公司之營運狀況，於借款期限內還清。
(2)日期:
借款日起一年內還清。
7.迄事實發生日為止，資金貸與餘額(仟元):
2062882
8.迄事實發生日為止，資金貸與餘額占公開發行公司最近期財務報表淨值之比率:
25.54
9.公司貸與他人資金之來源:
子公司本身
10.其他應敘明事項:
無</p>
</blockquote>
<p></br></p>
<p>我們能夠從重大消息公告當中得知公司的財務資訊, 交易對象等，甚至是該消息發布的緣由都會附上詳盡的文字敘述。而新聞媒體所發布的新聞稿在本篇分析當中將先不納入我們的資料來源，因為考量到不同媒體報導的角度可能不同，導致對於事件的描述上有所差異，用詞上情感的強烈亦會影響模型判斷的依據，徒增資料分析上的變因。</p>
<p></br></p>
<h1><font size = 5 ><strong>那我們如何將文字轉換為有意義的量值呢?</strong></font></h1>
<p>一個詞彙的涵義可能在不同使用者的經歷或是書寫的習慣上的差異，而使得語意上有所差距，因此如何使一個詞彙表達出表達出語意上真正的涵義，Tomas Mikolov 在 2013 年提出了他的看法：<font color = #FF4500 ><strong>字詞就如同我們人一般，能夠透過周遭相似的詞彙（context）反映出它真正的涵義。</strong></font> [<a href="https://arxiv.org/pdf/1301.3781.pdf">原文連結</a>]</p>
<p></br></p>
<p>概念如下圖：</p>
<p align="center">

  <img src="../extra/word_embedding.png">
  <font color = #AAAAAA> 將所有字詞以向量方式表示 ( 
  <a href="https://www.youtube.com/watch?v=UYPa347-DdE">圖片來源</a> )
  </font>

</p>

<p></br></p>
<h1><font size = 5 > <strong>雛型架構 Naive model</strong> </font></h1>
<blockquote>
<ol>
<li>Extractor 用來提取重大消息中有用的資訊</li>
<li>Classifier 將提取出的資訊結合股價區分為不同的類別，給予適當的數值範圍辨別漲跌的趨勢。</li>
</ol>
</blockquote>
<p>接下來就看我是怎麼實做這個部分的內容瞜</p>
<p></br></p>
<h1><font size = 5 > <strong>文章分類 Document Classification</strong> </font></h1>
<p>在斷詞的使用上我是透過 <a href="https://github.com/fxsjy/jieba">jieba</a> 的 <code>search_mode</code>。</p>
<p>以下方的例子為例，將 <code>text = "101在台北市信義區"</code>，可以很明顯的看出不同模式在斷詞結果上的差別。</p>
<ul>
<li>search_mode </br>
101 / 在 / 台北 / 北市 / 台北市 / 信義區</li>
<li>full_mode </br>
101 / 在 / 台北 / 台北市 / 北市 / 信 / 義 / 區</li>
<li>precise_mode </br>
101 / 在 / 台北 / 北市 / 台北市 / 信義區</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;101在台北市信義區&quot;</span>
<span class="k">print</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut_for_search</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>       <span class="c1"># serch_mode</span>
<span class="c1"># [&#39;101&#39;, &#39;在&#39;, &#39;台北&#39;, &#39;北市&#39;, &#39;台北市&#39;, &#39;信義區&#39;]</span>

<span class="k">print</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">cut_all</span> <span class="o">=</span> <span class="bp">True</span><span class="p">))</span>  <span class="c1"># full_mode</span>
<span class="c1"># [&#39;101&#39;, &#39;在&#39;, &#39;台北&#39;, &#39;台北市&#39;, &#39;北市&#39;, &#39;信&#39;, &#39;義&#39;, &#39;區&#39;]</span>

<span class="k">print</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">cut_all</span> <span class="o">=</span> <span class="bp">False</span><span class="p">))</span> <span class="c1"># precise_mode (default mode)</span>
<span class="c1"># [&#39;101&#39;, &#39;在&#39;, &#39;台北市&#39;, &#39;信義區&#39;]</span>
</pre></div>


<p>選擇使用 <code>search_mode</code> 是為了提高模型的召回率（recall），而模型的準確率（precision）與召回率之間的權衡是未來我們在針對模型作微調以提升預測的準確率時能夠著墨的地方，但現在先讓我們粗步地完成整個模型後再回頭過來調校，有關這方面的討論可以參考以下文章： </br></p>
<ul>
<li><a href="https://www.ycc.idv.tw/confusion-matrix.html">如何辨別機器學習模型的好壞？秒懂Confusion Matrix</a> </br></li>
</ul>
<p>按照上述的方式我們將所有抓取到的玉晶光重大公開消息做為本篇文本分析的語料庫（corpus），在刪除停用詞（stopword）等一些常用的詞或沒有意義的符號，例如：<code>在</code>、<code>不外乎</code>、<code>不只</code>等這一類的詞，將所有的詞集合而成一個向量。</p>
<blockquote>
<p>101 / 台北 / 北市 / 台北市 / 信義區</p>
</blockquote>
<p>我們可以很直觀的認為如果一個詞很頻繁的在一篇重大消息當中出現，表示這個詞可能與這篇公開消息的關聯性越高，可以將其出現的頻率－詞頻（Term Frequency,　TF）透過下方的公式計算得出：</p>
<blockquote>
<p>字詞在文章中出現的次數 / 文章的總詞彙數</p>
</blockquote>
<p>但是今天如果有一個詞（ex. 撤資）它只出現在特定的幾篇相較於廣泛出現的用語（ex. 資金），前者的重要性以及獨特性應該要更高，這也是逆向文件頻率（Inverse Document Frequency, IDF）的概念，計算的公式為：</p>
<blockquote>
<p>log (字詞在文章中出現的次數 / 文章的總詞彙數)</p>
</blockquote>
<p>並將TF與IDF相乘得到該篇公開消息的特徵值。</p>
<p>接下來我們可以利用 <code>consine similarity</code> 來評估所有公開消息與以下三項類別的相似程度：</p>
<ul>
<li>
<p>上漲 （Up） </br>
 <code>當天股價漲幅</code> &ge; 1 % </br></p>
</li>
<li>
<p>持平 （Stay） </br>
-1 % &le; <code>當天股價漲幅</code> &le; 1 % </br></p>
</li>
<li>
<p>下跌 （Down） </br>
<code>當天股價漲幅</code> &le; -1 % </br></p>
</li>
</ul>
<p align="center">

  <img src="../extra/cosine-similarity.png">
  <font color = #AAAAAA> 計算文本的相似度時很常見的方式 ( 
  <a href="https://clay-atlas.com/blog/2020/03/26/cosine-similarity-text-count/">圖片來源</a> )
  </font>

</p>

<p></br></p>
<p>但是在TF-IDF的使用上有些<font color = #FF4500 > <strong>缺點</strong> </font>：</br></p>
<ol>
<li>字詞出現的頻率較高不一定就能完整的表達該消息欲傳達的內容。</li>
<li>無法辨識同義詞，甚至是，甚至是同一個詞的變形也不能處理。</li>
<li>多義詞亦是 TF-IDF 無法處理的狀況。</li>
<li>存在特徵稀疏（Sparse feature）的問題。 （<a href="https://www.zhihu.com/question/31951092">參考文章</a>）</li>
</ol>
<p>若是結合文章標註 Document tag （<code>TF-IDF + Word2Vec</code>）及 <code>cosine similarity</code> 的話，雖然能包含文檔中的上下文，使其對於同義詞的辨識度提高，<font color = #FF4500 > <strong>但是同樣對於多義詞的辨識束手無策</strong> </font>。 </p>
<p></br></p>
<h1><font size = 5 > <strong>語言的博大精深！！</strong> </font></h1>
<p>舉個有趣的例子為例:</p>
<blockquote>
<p><font size = 4 > <strong>I arrived at the <font color = #FF4500 > bank </font> after crossing the river.</strong> </font><br>
<font size = 4 > <strong>I arrived at the <font color = #FF4500 > bank </font> after crossing the street.</strong> </font> </p>
</blockquote>
<p>當這兩句併排在一起我們能很明顯的判別出 <font color = #FF4500 > 
<strong>bank</strong> </font> 這一個字分別有不同的意思： </p>
<p align="center">

  <img src="../extra/bank_translate.png">
  <font color = #AAAAAA> 同一個詞在不同的句子當中會有不同的意義 ( 
  <a href="https://dictionary.cambridge.org/zht/%E8%A9%9E%E5%85%B8/%E8%8B%B1%E8%AA%9E-%E6%BC%A2%E8%AA%9E-%E7%B9%81%E9%AB%94/bank">圖片來源</a> )
  </font>

</p>

<p></br></p>
<p>而判斷的基準則是必須透過上下文的方式來判斷這個單字真正的涵義，這樣的現象在中文當中同樣也可以發現到。</p>
<p></br></p>
<h1><font size = 5 > <strong>類神經網路（Neural Network）</strong> </font></h1>
<p>近年來深度學習快速的發展，現在已經能夠將一個詞投影到較低維的空間當中，使得近似詞能夠獲得上下文資訊，透過非監督式學習（unsupervised learning）提取特徵，能廣泛的利用在其他的狀況（generalization）。
依照提取出來的特徵做 expression，再根據不同的 case 微調（fine tuning）做監督式學習（supervised learning），來達成我們要的目的。</p>
<p></br></p>
<h2><font size = 5 > <strong>BERT</strong> </font></h2>
<p>最終我決定以 GOOGLE 在 2018 年發表的自然語言模型－ BERT 作為我們分類器（Classifier）的最終 model，在 <a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb">colab</a> 可以看到 GOOGLE 自附的所有教學以及完整的 code，若要自行搭載環境也可以參考以下文章：</p>
<ul>
<li><a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html">進擊的 BERT：NLP 界的巨人之力與遷移學習</a> </br></li>
</ul>
<p>BERT整個 model 最重要的部分是以 <code>transformer</code> 中 <font color = #FF4500 > <strong>Attention</strong> </font> 的機制所組成，原因是考慮了過去常用的類神經網路－ RNN是依照時間的次序（time series）由左往右計算，這對模型產生了一定程度上的限制。
在 <code>transformer</code> 作者的研究論文當中給予的定義也提到：</p>
<blockquote>
<p>Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.</p>
</blockquote>
<p align="center">

  <img src="../extra/rnn-animate.gif">
  <font color = #AAAAAA> </br> RNN模型中需要一步步地按照順序從頭處理所有詞語 ( 
  <a href="https://www.youtube.com/watch?time_continue=2&v=LHXXI4-IEns">圖片來源</a> )
  </font>

</p>

<p>以前面提到的英文例句為例：</p>
<blockquote>
<p><font size = 4 > <strong>I arrived at the <font color = #FF4500 > bank </font> after crossing the river.</strong> </font> </p>
</blockquote>
<p>要解析出<font color = #FF4500 > <strong>bank</strong> </font>真正的涵義須完整閱讀到<font color = #FF4500 > <strong>river</strong> </font> ，如果這兩個詞語距離較遠時RNN的執行效率就會較差。
而 <font color = #FF4500 > <strong>Attention</strong> </font> 則是透過計算每個單詞與其他所有單詞之間的關聯性，能夠很快地得出在翻譯 bank 的同時， river 有較高的 Attention score ，就可以依此作加權，反饋至前一個神經網絡，能夠迅速且充分地掌握上下文信息的特徵。（<a href="https://zhuanlan.zhihu.com/p/47282410">參考文章</a>）</p>
<p align="center">

  <img src="../extra/self_attention.gif">
  <font color = #AAAAAA> </br> Self-Attention計算每個字詞與其他所有字詞之間的關係，掌握上下文的關聯性 ( 
  <a href="https://zhuanlan.zhihu.com/p/47282410">圖片來源</a> )
  </font>

</p>

<p>也因為 <code>transformer</code> 這樣的並行機制較符合現行GPU的框架，model 運行的速度亦能得到大幅地提升，如果想要更詳細地了解其運作原理可以閱讀 <a href="http://jalammar.github.io/"><strong>Jay Alammer</strong></a> 的文章，附有大量的圖文解說：</br>
* <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>
<p>BERT 詳細的原理可以參考李宏毅老師的影片以及其他網路上的資源：</br>　
* <a href="https://www.youtube.com/watch?v=UYPa347-DdE">ELMO, BERT, GPT</a> </br>
* <a href="https://www.jishuwen.com/d/2M6u/zh-tw">BERT模型詳解</a> </br></p>
<p>BERT提供了四種下游的任務供使用者選取</p>
<ul>
<li>Sentence Pair Cassification </br>
ex. 給定一對句子，目標是預測第二個句子相對第一個句子是包含、矛盾還是中立。</li>
<li>Single Sentence Classification </br>
ex. 從電影評論中提取句子或者是標註人類帶有情緒性的用詞。</li>
<li>Question Answering  </br>
ex. 給定一個問題以及包含答案的段落，預測答案落在文本的哪一範圍中。</li>
<li>Single Sentence Tagging </br>
ex. 識別文本內具有特別意義的字詞－人名、地名、專有名詞等。</li>
</ul>
<p align="center">

  <img src="../extra/BERT_tasks.png">
  <font color = #AAAAAA> </br> Self-Attention計算每個字詞與其他所有字詞之間的關係，掌握上下文的關聯性 ( 
  <a href="https://arxiv.org/pdf/1810.04805.pdf">圖片來源</a> )
  </font>

</p>

<p></br></p>
<p>依照我們的需求，我們使用 <a href="https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2">TensorFlow</a> 中與中文相關的 <font color = #FF4500 > <strong>pre-train model</strong> </font> 對我們整理好的資料集做下游的 <code>Single Sentence Classification</code> 監督式學習任務。</p>
<p align="center">

  <img src="../extra/3406_text.png">
  <font color = #AAAAAA> </br> 將玉晶光的歷史股價與日期所對應的公開消息整理成一個dataset 
  </font>

</p>

<p>首先，我們先將我們的原始資料集轉換成BERT能夠讀得懂的格式</p>
<ul>
<li>text_a </br>
我們需要被給定分類類別的文本。</li>
<li>text_b  </br>
ex. 給定一個問題以及包含答案的段落，預測答案落在文本的哪一範圍中。</li>
<li>label </br>
總共要區分為哪幾類。</li>
</ul>
<div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">上漲: 0</span>
<span class="sd">持平: 1</span>
<span class="sd">下跌: 2</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">data_column</span> <span class="o">=</span> <span class="s1">&#39;Text&#39;</span>
<span class="n">label_column</span> <span class="o">=</span> <span class="s1">&#39;Signal&#39;</span>

<span class="n">train_InputExamples</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">bert</span><span class="o">.</span><span class="n">run_classifier</span><span class="o">.</span><span class="n">InputExample</span><span class="p">(</span>
                                                                   <span class="n">text_a</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">[</span><span class="n">data_column</span><span class="p">],</span> 

       <span class="n">label</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">[</span><span class="n">label_column</span><span class="p">]),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">test_InputExamples</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">bert</span><span class="o">.</span><span class="n">run_classifier</span><span class="o">.</span><span class="n">InputExample</span><span class="p">(</span>
                                                                   <span class="n">text_a</span> <span class="o">=</span> <span class="n">test_set</span><span class="p">[</span><span class="n">data_column</span><span class="p">],</span> 

                                                                   <span class="n">label</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">[</span><span class="n">label_column</span><span class="p">]),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">BERT_MODEL_HUB</span> <span class="o">=</span> <span class="s2">&quot;https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2&quot;</span>

<span class="k">def</span> <span class="nf">create_tokenizer_from_hub_module</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Get the vocab file and casing info from the Hub module.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="n">bert_module</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">Module</span><span class="p">(</span><span class="n">BERT_MODEL_HUB</span><span class="p">)</span>
        <span class="n">tokenization_info</span> <span class="o">=</span> <span class="n">bert_module</span><span class="p">(</span><span class="n">signature</span><span class="o">=</span><span class="s2">&quot;tokenization_info&quot;</span><span class="p">,</span> <span class="n">as_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
            <span class="n">vocab_file</span><span class="p">,</span> <span class="n">do_lower_case</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">tokenization_info</span><span class="p">[</span><span class="s2">&quot;vocab_file&quot;</span><span class="p">],</span><span class="n">tokenization_info</span><span class="p">[</span><span class="s2">&quot;do_lower_case&quot;</span><span class="p">]])</span>

    <span class="k">return</span> <span class="n">bert</span><span class="o">.</span><span class="n">tokenization</span><span class="o">.</span><span class="n">FullTokenizer</span><span class="p">(</span><span class="n">vocab_file</span><span class="o">=</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="n">do_lower_case</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">create_tokenizer_from_hub_module</span><span class="p">()</span>

<span class="c1"># We&#39;ll set sequences to be at most 128 tokens long.</span>
<span class="n">MAX_SEQ_LENGTH</span> <span class="o">=</span> <span class="mi">128</span>
<span class="c1"># Convert our train and test features to InputFeatures that BERT understands.</span>
<span class="n">train_features</span> <span class="o">=</span> <span class="n">bert</span><span class="o">.</span><span class="n">run_classifier</span><span class="o">.</span><span class="n">convert_examples_to_features</span><span class="p">(</span><span class="n">train_InputExamples</span><span class="p">,</span> <span class="n">label_list</span><span class="p">,</span> <span class="n">MAX_SEQ_LENGTH</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">test_features</span> <span class="o">=</span> <span class="n">bert</span><span class="o">.</span><span class="n">run_classifier</span><span class="o">.</span><span class="n">convert_examples_to_features</span><span class="p">(</span><span class="n">test_InputExamples</span><span class="p">,</span> <span class="n">label_list</span><span class="p">,</span> <span class="n">MAX_SEQ_LENGTH</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
</pre></div>


<p><a href="https://www.kesci.com/home/project/5be7e948954d6e0010632ef2">文本分類算法錦集</a> </br>
<a href="https://taweihuang.hpd.io/2017/03/01/tfidf/">TF-IDF 演算法：快速計算單字與文章的關聯</a> </br>
<a href="https://zhuanlan.zhihu.com/p/152526817">TFIDF+Wordembedding无监督多标签文本分类算法（论文解读)</a> </br>
<a href="https://zhuanlan.zhihu.com/p/102208639">BERT下游任務</a></p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/natural-language-processing.html">Natural Language Processing</a>
      <a href="/tag/stock.html">Stock</a>
    </p>
  </div>





</article>

    <footer>
<p>&copy; Michael Chien 2020</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Learning note & Build some side projects | Michael Chien ",
  "url" : "",
  "image": "static/profile.jpg",
  "description": ""
}
</script>


</body>
</html>